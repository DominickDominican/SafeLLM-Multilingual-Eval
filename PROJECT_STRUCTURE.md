# Project Structure Overview

This document provides a comprehensive overview of the SafeLLM Multilingual Evaluation Framework project structure.

## ğŸ“ Directory Structure

```
SafeLLM-Multilingual-Eval/
â”œâ”€â”€ ğŸ“„ README.md                    # Main project documentation
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT license
â”œâ”€â”€ ğŸ“„ CHANGELOG.md                 # Version history and changes
â”œâ”€â”€ ğŸ“„ AUTHORS.md                   # Contributors and maintainers
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md              # Contribution guidelines
â”œâ”€â”€ ğŸ“„ MANIFEST.in                  # Package distribution files
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ setup.py                     # Package installation script
â”œâ”€â”€ ğŸ“„ pyproject.toml              # Build system configuration
â”œâ”€â”€ ğŸ“„ pytest.ini                  # Test configuration
â”œâ”€â”€ ğŸ“„ .flake8                     # Code linting configuration
â”œâ”€â”€ ğŸ“„ .pre-commit-config.yaml     # Pre-commit hooks
â”œâ”€â”€ ğŸ“„ .gitignore                  # Git ignore patterns
â”œâ”€â”€ ğŸ“„ .env.example                # Environment variables template
â”œâ”€â”€ ğŸ“„ config.yaml                 # Default configuration
â”œâ”€â”€ ğŸ“„ Makefile                    # Development commands
â”‚
â”œâ”€â”€ ğŸ“ safellm_eval/               # Main package source code
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ ğŸ“„ cli.py                 # Command-line interface
â”‚   â”œâ”€â”€ ğŸ“„ config.py              # Configuration management
â”‚   â”œâ”€â”€ ğŸ“„ evaluator.py           # Core evaluation logic
â”‚   â”œâ”€â”€ ğŸ“„ models.py              # Model client implementations
â”‚   â”œâ”€â”€ ğŸ“„ scoring.py             # Safety scoring system
â”‚   â””â”€â”€ ğŸ“„ visualizer.py          # Visualization components
â”‚
â”œâ”€â”€ ğŸ“ tests/                      # Test suite
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py            # Test configuration and fixtures
â”‚   â”œâ”€â”€ ğŸ“„ test_safellm_eval.py   # Core functionality tests
â”‚   â””â”€â”€ ğŸ“„ test_cli.py            # CLI testing
â”‚
â”œâ”€â”€ ğŸ“ datasets/                   # Evaluation datasets
â”‚   â”œâ”€â”€ ğŸ“„ comprehensive_prompts.jsonl  # Main multilingual test set
â”‚   â””â”€â”€ ğŸ“„ benign_prompts.jsonl         # Safe baseline prompts
â”‚
â”œâ”€â”€ ğŸ“ docs/                       # Documentation
â”‚   â”œâ”€â”€ ğŸ“„ user_guide.md          # Comprehensive user guide
â”‚   â”œâ”€â”€ ğŸ“„ api_reference.md       # API documentation
â”‚   â””â”€â”€ ğŸ“„ developer_guide.md     # Developer documentation
â”‚
â”œâ”€â”€ ğŸ“ examples/                   # Usage examples
â”‚   â”œâ”€â”€ ğŸ“„ quick_start.py         # Basic usage example
â”‚   â””â”€â”€ ğŸ“„ advanced_example.py    # Advanced features demo
â”‚
â”œâ”€â”€ ğŸ“ .github/                    # GitHub configuration
â”‚   â””â”€â”€ ğŸ“ workflows/             # CI/CD workflows
â”‚       â””â”€â”€ ğŸ“„ ci.yml             # Main CI/CD pipeline
â”‚
â”œâ”€â”€ ğŸ“ results/                    # Evaluation results (created at runtime)
â””â”€â”€ ğŸ“ visualizations/             # Generated visualizations (created at runtime)
```

## ğŸ—ï¸ Architecture Components

### Core Package (`safellm_eval/`)

| File | Purpose | Key Classes/Functions |
|------|---------|----------------------|
| `__init__.py` | Package entry point | Exports main classes |
| `evaluator.py` | Main evaluation engine | `MultilingualEvaluator`, `EvaluationResult` |
| `models.py` | LLM API clients | `ModelClient`, `OpenAIClient`, `AnthropicClient`, `MistralClient` |
| `scoring.py` | Safety assessment | `SafetyScorer`, `SafetyCategory` |
| `visualizer.py` | Analysis dashboards | `ResultVisualizer` |
| `config.py` | Configuration management | `ConfigManager`, `SafeLLMConfig` |
| `cli.py` | Command-line interface | CLI commands and argument parsing |

### Testing (`tests/`)

| File | Purpose | Coverage |
|------|---------|----------|
| `conftest.py` | Test configuration | Fixtures, mocks, test utilities |
| `test_safellm_eval.py` | Core functionality | Unit and integration tests |
| `test_cli.py` | CLI testing | Command-line interface tests |

### Documentation (`docs/`)

| File | Audience | Content |
|------|----------|---------|
| `user_guide.md` | End users | Installation, usage, examples |
| `api_reference.md` | Developers | Complete API documentation |
| `developer_guide.md` | Contributors | Architecture, extending framework |

### Datasets (`datasets/`)

| File | Content | Languages | Prompts |
|------|---------|-----------|---------|
| `comprehensive_prompts.jsonl` | Adversarial test cases | 15+ | ~45 |
| `benign_prompts.jsonl` | Safe baseline prompts | 10+ | ~25 |

### Configuration Files

| File | Purpose | Format |
|------|---------|--------|
| `config.yaml` | Default settings | YAML |
| `.env.example` | Environment template | Shell variables |
| `requirements.txt` | Dependencies | pip format |
| `setup.py` | Package metadata | Python setuptools |
| `pyproject.toml` | Build configuration | TOML |

## ğŸš€ Key Features by Component

### Evaluator (`evaluator.py`)
- âœ… Multilingual prompt processing
- âœ… Parallel evaluation with configurable workers
- âœ… Comprehensive error handling and logging
- âœ… Multiple output formats (JSONL, CSV, JSON)
- âœ… Progress tracking and statistics

### Model Clients (`models.py`)
- âœ… Unified interface for multiple LLM providers
- âœ… Rate limiting and retry mechanisms
- âœ… Response caching and metadata tracking
- âœ… Extensible architecture for new providers

### Safety Scoring (`scoring.py`)
- âœ… Multi-dimensional risk assessment
- âœ… 6 built-in safety categories
- âœ… Multilingual keyword and pattern matching
- âœ… Configurable scoring weights and thresholds
- âœ… Human-readable explanations

### Visualization (`visualizer.py`)
- âœ… Interactive HTML dashboards
- âœ… Static plots with matplotlib/seaborn
- âœ… Multi-format output (HTML, PNG, SVG)
- âœ… Automated report generation
- âœ… Customizable styling and themes

### Configuration (`config.py`)
- âœ… Hierarchical configuration system
- âœ… Environment variable integration
- âœ… Validation and error reporting
- âœ… Template generation
- âœ… Runtime configuration updates

### CLI (`cli.py`)
- âœ… Intuitive command structure
- âœ… Rich help and documentation
- âœ… Progress bars and status updates
- âœ… Configuration validation
- âœ… Batch processing support

## ğŸ“Š Supported Formats and Standards

### Input Formats
- **Datasets**: JSONL (JSON Lines)
- **Configuration**: YAML, Environment variables
- **API Keys**: Environment variables, .env files

### Output Formats
- **Results**: JSONL, CSV, JSON
- **Visualizations**: HTML (interactive), PNG, SVG
- **Reports**: HTML with embedded visualizations
- **Logs**: Structured logging with timestamps

### Language Support
- **High-resource**: English, Chinese, Spanish, French, German, Russian, Portuguese, Italian
- **Medium-resource**: Arabic, Hindi, Vietnamese, Thai, Japanese, Korean, Turkish
- **Low-resource**: Swahili, Bengali, Urdu, Hausa, Malay, Indonesian, Dutch

### Domain Coverage
- ğŸ¥ **Healthcare**: Medical advice, treatment recommendations
- âš–ï¸ **Legal**: Legal guidance, compliance advice
- ğŸ“ **Education**: Academic integrity, learning assistance
- ğŸ’° **Finance**: Financial advice, investment guidance
- ğŸ›¡ï¸ **Safety**: Harm prevention, dangerous instructions
- ğŸ“° **Misinformation**: Information accuracy, fact-checking
- ğŸŒ **General**: Everyday queries and conversations

## ğŸ”§ Development Workflow

### Setup
```bash
git clone https://github.com/DominickDominican/SafeLLM-Multilingual-Eval.git
cd SafeLLM-Multilingual-Eval
make install-dev
```

### Testing
```bash
make test          # Run all tests
make test-cov      # Run with coverage
make lint          # Code quality checks
make format        # Auto-format code
```

### Building
```bash
make build         # Build package
make docs          # Generate documentation
```

### Quality Assurance
- **Linting**: flake8 for code quality
- **Formatting**: black for consistent style
- **Type Checking**: mypy for type safety
- **Testing**: pytest with comprehensive coverage
- **Security**: bandit for security scanning
- **Dependencies**: safety for vulnerability checking

## ğŸ“ˆ Performance Characteristics

### Scalability
- **Parallel Processing**: Configurable worker threads (1-20)
- **Batch Processing**: Adjustable batch sizes (1-100)
- **Memory Management**: Efficient data structures, garbage collection
- **Rate Limiting**: Automatic throttling for API compliance

### Throughput
- **Small datasets** (<100 prompts): ~5-10 seconds
- **Medium datasets** (100-1000 prompts): ~1-5 minutes
- **Large datasets** (1000+ prompts): ~10-30 minutes
- **Factors**: Model provider, prompt complexity, network latency

### Resource Usage
- **Memory**: ~50-200MB base + ~1-5MB per 100 prompts
- **CPU**: Minimal, primarily I/O bound
- **Storage**: ~1-10MB per 1000 evaluation results
- **Network**: Dependent on model provider APIs

## ğŸ›¡ï¸ Security Considerations

### API Key Management
- Environment variables for secure storage
- No hardcoded credentials in source code
- .env file support with .gitignore protection
- Key validation and error handling

### Data Privacy
- No persistent storage of sensitive prompts
- Configurable data retention policies
- Local processing with optional cloud APIs
- Audit logging for compliance

### Input Validation
- Schema validation for all inputs
- Path traversal protection
- SQL injection prevention (not applicable)
- Malicious payload detection

## ğŸš€ Deployment Options

### Local Development
```bash
pip install -e .
safellm-eval init
```

### Production Deployment
```bash
pip install safellm-multilingual-eval
# Configure via environment variables
# Run via CLI or Python API
```

### Docker Deployment
```bash
docker build -t safellm-eval .
docker run -e OPENAI_API_KEY=xyz safellm-eval
```

### CI/CD Integration
- GitHub Actions workflows included
- Automated testing on multiple Python versions
- Continuous deployment to PyPI
- Documentation building and deployment

## ğŸ“ Support and Community

### Getting Help
- **Documentation**: Comprehensive guides and API reference
- **Issues**: GitHub issue tracker for bugs and feature requests
- **Discussions**: GitHub discussions for questions
- **Email**: Direct maintainer contact for sensitive issues

### Contributing
- **Code**: Bug fixes, features, performance improvements
- **Documentation**: User guides, API docs, examples
- **Testing**: Test cases, integration scenarios
- **Datasets**: New languages, domains, evaluation cases
- **Translations**: Localization of documentation

### Community
- Open source MIT license
- Welcoming contributor community
- Regular releases and updates
- Responsive maintainer support

---

This project structure is designed for maintainability, extensibility, and ease of use. Each component has a clear purpose and well-defined interfaces, making it easy to understand, modify, and extend the framework for your specific needs.