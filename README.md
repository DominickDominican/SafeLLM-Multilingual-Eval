
# SafeLLM-Multilingual-Eval

A multilingual evaluation framework for testing the safety, robustness, and alignment of large language models (LLMs) across high-stakes domains.

ä¸€ä¸ªç”¨äºå¤šè¯­è¨€åœºæ™¯ä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨æ€§ã€ç¨³å¥æ€§ä¸å¯¹é½æ€§è¯„ä¼°çš„å¼€æºæ¡†æ¶ï¼Œèšç„¦åŒ»ç–—ã€æ³•å¾‹ã€æ•™è‚²ç­‰é«˜é£é™©åº”ç”¨é¢†åŸŸã€‚

## ğŸ” Overview / é¡¹ç›®ç®€ä»‹

This project provides a testbed of adversarial and safety-critical prompts in over 12 languages, targeting common failure modes in multilingual deployments of LLMs like Claude, GPT-4, and Mistral.

è¯¥æ¡†æ¶æ¶µç›–12ç§ä»¥ä¸Šè¯­è¨€ï¼Œå†…ç½®å¯¹æŠ—æ€§ä¸å®‰å…¨å…³é”®å‹æç¤ºï¼Œä¸“æ³¨äºè¯„ä¼°Claudeã€GPT-4ã€Mistralç­‰ä¸»æµå¤§æ¨¡å‹åœ¨å¤šè¯­è¨€åœºæ™¯ä¸­çš„è„†å¼±ç‚¹ä¸è¡Œä¸ºåå·®ã€‚

## ğŸ¯ Objectives / ç ”ç©¶ç›®æ ‡

- Evaluate LLM safety across low-resource languages  
- Test prompt injection and jailbreak behavior across languages  
- Compare alignment responses under semantic equivalents  
- Support reproducibility and open-source benchmarking

- è¯„ä¼°ä½èµ„æºè¯­è¨€ä¸‹çš„æ¨¡å‹å®‰å…¨è¡¨ç°  
- è·¨è¯­è¨€æµ‹è¯•æç¤ºæ³¨å…¥ä¸è¶Šç‹±é£é™©  
- å¯¹æ¯”è¯­ä¹‰ç­‰ä»·è¡¨è¾¾ä¸‹çš„å¯¹é½å“åº”ä¸€è‡´æ€§  
- æä¾›å¯é‡å¤ã€å¯å¼€æºçš„å¯¹é½åŸºå‡†æµ‹è¯•å·¥å…·

## ğŸ§ª Evaluation Domains / è¯„ä¼°åœºæ™¯

- ğŸ¥ Healthcare (e.g. triage decision errors)  
- âš–ï¸ Legal assistance (e.g. biased advice)  
- ğŸ“ Education (e.g. misalignment in guidance)  
- ğŸŒ Cross-lingual policy compliance

## ğŸ§° Features / æ ¸å¿ƒåŠŸèƒ½

- âœ… Multilingual adversarial prompt set  
- âœ… Model response collection interface  
- âœ… Safety scoring + visualization  
- âœ… Support for Claude, OpenAI, Mistral APIs

## ğŸ“‚ Repository Structure / é¡¹ç›®ç»“æ„

```
ğŸ“ datasets/             # Multilingual prompts  
ğŸ“ evaluation/           # LLM scoring scripts  
ğŸ“ visualizations/       # Plots + dashboards  
ğŸ“„ config.yaml           # Model + language settings  
ğŸ“„ README.md
```

## ğŸ“œ License

MIT License

## ğŸ”— Maintainer / é¡¹ç›®è´Ÿè´£äºº  
**Dominick Dominican**  
Email: dominickdominican47@gmail.com
